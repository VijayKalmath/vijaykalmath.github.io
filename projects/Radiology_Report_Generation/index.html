<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="SUIWaGS_bqL-07kpchrmdlwKL6vkr5JXP8xNpoGHnIU"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Radiology Report Generation for Chest X-RAY Images | Vijay S. Kalmath</title> <meta name="author" content="Vijay S. Kalmath"/> <meta name="description" content="Analyze and Improve Resnet-Bert based MultiModal-PrototypeNetwork Model for Radiology Report Generation."/> <meta name="keywords" content="kalmath,Vijay Columbia, vijay,Vijay,Kalmath,Vijay Kalmath,vijay kalmath,vijaykalmath,machine-learning,deep-learning,columbia university machine learning,machine learning research assistant,deep learning,deep learning research assistant,data-science,Adversarial Training, Model-Compression,K-Branch CNN, BigEarthNet,Vijay EDAV,Vijay Cisco,Vijay ACI,AImodelshare"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/deep_learning.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://vijaykalmath.github.io/projects/Radiology_Report_Generation/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://vijaykalmath.github.io/">Vijay S. Kalmath</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Github</a> </li> <li class="nav-item"> <a class="nav-link active" href="https://vijaykalmath.github.io/assets/pdf/Kalmath,Vijay_Resume_2023.pdf">Resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Radiology Report Generation for Chest X-RAY Images</h1> <p class="post-description">Analyze and Improve Resnet-Bert based MultiModal-PrototypeNetwork Model for Radiology Report Generation.</p> </header> <article> <p>The project is based on the the 2022 Paper by Jun Wang et.al. <a href="https://arxiv.org/pdf/2207.04818.pdf" target="_blank" rel="noopener noreferrer">Arxiv Link</a></p> <p>Our work with XProNet mainly focused on building an inference pipeline and analyzing the generated texts from XProNets for radiology images for Accenture.</p> <p>Radiography is the most well established imaging method to diagnose diseases. The ability to automatically generate accurate medical reports from diagnostic images saves time and efforts of medical experts and aids the increase of healthcare accessibility.</p> <p>A multi-modal deep learning model can embed extracted visual (from x-ray images) and textual (from text reports) features in common space and learn the cross-modal patterns to generate a textual report that describes the radiology image. This project analyzes the SOTA system for this task XProNet.</p> <h2 id="dataset">Dataset</h2> <p>The IU-Xray dataset contains 7,470 chest x-ray images with 3,955 corresponding medical reports published by Indiana University.</p> <p>The Dataset similar to other medical datasets has an inherent imbalance as :</p> <ol> <li>Most reports associated to the images are normal, having no findings.</li> <li>Even in abnormal reports, affected regions only exist in small parts of the text &amp; image</li> </ol> <p>Pseudo Label Generation Cross-modal prototypes require category information for each sample that corresponds to the 14 different chest disorders in literature and is often not provided in the datasets. To address this problem for prototype learning, the XProNet authors utilize CheXbert, an automatic radiology report labeler, to generate a pseudo label for each image-text pair.</p> <div class="row"> <div class="col-md-6 mt-2 mt-md-0 "> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rrg/example-dataset-entry-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rrg/example-dataset-entry-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rrg/example-dataset-entry-1400.webp"></source> <img src="/assets/img/rrg/example-dataset-entry.png" class="img-fluid rounded" width="auto" height="auto" title="IU-Xray Dataset" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rrg/data-frequency-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rrg/data-frequency-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rrg/data-frequency-1400.webp"></source> <img src="/assets/img/rrg/data-frequency.png" class="img-fluid rounded" width="auto" height="auto" title="IU_XRay Data Imbalance" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 1. Example of frontal (left) and lateral (right) images for a given patient 2. Frequency of pseudo-labels in IU-X Ray Dataset </div> <h2 id="xpronet-architecture-and-performance">XProNet Architecture and Performance</h2> <p>Before delving into the analysis of XProNet and our project centered around this model, it is essential to understand the architecture of XProNet.</p> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0 "> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rrg/architecture-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rrg/architecture-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rrg/architecture-1400.webp"></source> <img src="/assets/img/rrg/architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="XProNet Architecture" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0 "> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rrg/rrg-model-output-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rrg/rrg-model-output-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rrg/rrg-model-output-1400.webp"></source> <img src="/assets/img/rrg/rrg-model-output.jpg" class="rounded z-depth-1" width="auto" height="315px" title="XProNet Architecture Training" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 3. XProNet Architecture (architecture from paper by Jun Wang and co) 4. Generated reports across training epochs. </div> <p>The XProNet model architecture comprises three key components:</p> <ol> <li> <p>Image Feature Extractor :</p> <p>ResNet-101 is utilized as the image feature extractor in XProNet. It extracts informative features from Chest X-ray images at the last convolutional layer before the final average pooling operation.</p> </li> <li> <p>Cross-modal Prototype Network:</p> <p>Cross-modal learning enables joint learning of image and text representations. XProNet incorporates a matrix that learns and stores cross-modal patterns, serving as intermediate representations. Cross-modal prototype querying measures similarity between single-modal representations and cross-modal prototype vectors under the same label. The top-ranked vectors are selected and interact with single-modal representations in a weighted-sum manner.</p> </li> <li> <p>Report Generator Transformer The report generation is performed by a Transformer, which consists of encoder and decoder blocks with multiheaded attention. Visual features from querying and responding are fed into the Encoder. The fused output of the Encoder, combined with textual features, is input to the Decoder to predict the current output text.</p> </li> </ol> <h2 id="project-work">Project Work</h2> <p>As mentioned before, our work with XProNet mainly focused on building an inference pipeline and analyzing the generated texts from XProNets for radiology images.</p> <p><ins> Inference Pipeline</ins></p> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0 "> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rrg/inference-pipeline-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rrg/inference-pipeline-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rrg/inference-pipeline-1400.webp"></source> <img src="/assets/img/rrg/inference-pipeline.png" class="img-fluid rounded" width="auto" height="auto" title="XProNet Architecture" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0 "> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rrg/inference-pipeline-results-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rrg/inference-pipeline-results-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rrg/inference-pipeline-results-1400.webp"></source> <img src="/assets/img/rrg/inference-pipeline-results.png" class="img-fluid rounded" width="auto" height="auto" title="XProNet Architecture" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rrg/inference-pipeline-table-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rrg/inference-pipeline-table-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rrg/inference-pipeline-table-1400.webp"></source> <img src="/assets/img/rrg/inference-pipeline-table.png" class="img-fluid rounded" width="auto" height="auto" title="XProNet Architecture" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 4. XProNet Training vs Inference Pipeline 5. Comparison of BLEU scores with new Inference pipeline and table </div> <p>The XProNet system utilizes a cross-modal prototype matrix to store the relationships between image features and text features. This matrix is also dependent on a multi-label vector, referred to as the pseudo-label, which identifies the presence of 14 different chest diseases. The pseudo-label is generated by feeding the natural language report through the CheXBert model. Regrettably, this interdependence between the natural language report and the generation of pseudo-labels renders the XProNet system inapplicable for inference for health applications.</p> <p>We build the inference pipeline with a component we call the Pseudo-label Generator. This component is capable of generating pseudo-labels from chest X-ray images through a multi-label classification task, effectively identifying the presence of each of the 14 different chest diseases. The Pseudo-label Generator serves as a robust replacement for the CheXBert module, as it operates independently of the natural language report.</p> <p>To construct the Pseudo-label Generator multi-label classification model, we utilize a ResNet-101 to extract visual features from Chest X-ray images. The final layer of the ResNet-101 is replaced with a dense layer of size 14, equipped with a sigmoid activation function, to output the binary values associated with the 14 disorders. We use the IU-XRay images and the pseudo-labels generated by CheXBert as the target for training. We follow the same data splits proportions as XProNet to divide the IU-Xray dataset into train (70%), validation (10%) and test (20%) sets. The model is trained for 30 epochs using the Adam optimizer and a step-wise learning rate scheduler.</p> <p>The comparison of performance with the new inference pipeline with the Pseudo-Label Generator indicates a very slight decrease in performance on the test set for IU-Xray.This indicates that the Inference Pipeline built is almost as good as the original architecture proposed while eliminating the dependency of Radiology Reports much before generating them from the Chest X-ray images</p> <p><ins> Mode Collapse</ins></p> <p>One of the notable observations during training of the XProNet model was that the model generated reports which were similar in appearance for multiple images in the test and validation data set.</p> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0 "> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rrg/mode-collapse-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rrg/mode-collapse-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rrg/mode-collapse-1400.webp"></source> <img src="/assets/img/rrg/mode-collapse.png" class="img-fluid rounded" width="auto" height="auto" title="XProNet Architecture" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 6. Frequency of unique reports generated for the test data set </div> <p>Figure 6 illustrates the frequency of unique generated reports by the XProNet for the test IU-XRay images. While the test dataset contains <ins> 396 unique ground truth natural language reports, the XProNet model only generates 18 unique reports </ins>, which is indicative of a Mode Collapse issue. This suggests that the model is unable to generate diverse reports for the test dataset.</p> <p>A closer examination of the generated reports reveals that the <ins> 18 unique reports only contain 37 unique words </ins>, indicating that the unique reports are not significantly different from one another and only vary by a few words or word order. In comparison, the ground truth reports contain <ins> 634 unique words </ins>. This drastic decrease in the number of unique words suggests that the XProNet model is unable to capture all of the natural language information and is in- stead defaulting to a limited set of words and reports. This further supports the conclusion that the model is experiencing Mode Collapse.</p> <p>We found through experimentation with differing inputs that for two different radiology images with different issues, if we use a single pseudo-label then the reports generated are exactly the same indicating that the transformers are over-dependent on the pseudo=labels.</p> <p>Further works to understand this dependence on pseudo-label generation needs to be analyzed along with prototype matrix analysis.</p> <p>Team Members : Vijay S Kalmath, Amrutha Varshini Sundar, Andrew Schaefer, Ayush Sinha, Prabha Kiranmai Vasireddy, Navjot Singh.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Vijay S. Kalmath. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: December 12, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L767ZJE8EN"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L767ZJE8EN");</script> </body> </html>